# -*- coding: utf-8 -*-
"""LDAdeployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qSY1JU60ZFlDCuFjI6cKt9HvkCWEmC4s
"""

import pandas as pd
import numpy as np
import re
import string
import spacy
import re
import gensim
from gensim import corpora
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('wordnet')
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from textblob import Word
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression


def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
def tokenization(text):
    tokens = re.split('W+',text)
    return tokens
def remove_stopwords(text):
    stop_words = stopwords.words('english')
    new_words=['la','pa','ya']
    stop_words.extend(new_words)
    textArr = text.split(' ')
    rem_text = " ".join([i for i in textArr if i not in stop_words])
    return rem_text
def stemming(text):
    #defining the object for stemming
    porter_stemmer = PorterStemmer() 
    stem_text = [porter_stemmer.stem(text) ]
    return stem_text
def lemmatizer(text):
    #defining the object for Lemmatization
    wordnet_lemmatizer = WordNetLemmatizer()
    lemm_text = [wordnet_lemmatizer.lemmatize(text) ]
    return lemm_text

#run this function to get the topics as output
#input_data: is a string of the text taken from google doc
#return type: string ,the string contains the topic
def topic_predictor(input_data):
  data= pd.DataFrame()
  data['Data']=[input_data]
  #storing the puntuation free text
  data['clean_msg']= data['Data'].apply(lambda x:remove_punctuation(x))
  data['msg_lower']= data['clean_msg'].apply(lambda x: x.lower())
  #storing the tokenized text
  data['msg_tokenied']= data['msg_lower'].apply(lambda x: tokenization(x))
  #removing stop words
  data['no_stopwords']= data['msg_tokenied'].apply(lambda x:remove_stopwords(x[0]))
  #stemming of data
  data['msg_stemmed']=data['no_stopwords'].apply(lambda x: stemming(x))
  #lemmatizing the data
  data['msg_lemmatized']=data['no_stopwords'].apply(lambda x:lemmatizer(x))
  dictionary = corpora.Dictionary(data ['msg_lemmatized'])
  doc_term_matrix = [dictionary.doc2bow(rev) for rev in data ['msg_lemmatized']]
  lda_model = gensim.models.ldamodel.LdaModel
  # Loading a model
  lda_model = lda_model.load("/home/ubuntu/Emotion-Interface/topic-modelling/model-files/Model") #change the path to model file path
  op_topics=lda_model.get_document_topics(doc_term_matrix[0],minimum_probability=0.01)
  Topics=["Budget/Pricing/Product","Compatibility/Security/Customization","Customization/Marketing/Budget","Quality/Pricing/Customization","Customization/Compatibility/Services","Security/Price/Customization","Customization/Compatibility/Marketing","Compatibility/Security","Customer service/Quality/Marketing","Customization/Marketing"]
  k=dict(op_topics)
  dict1 = k
  sorted_tuples = sorted(dict1.items(), key=lambda item: item[1], reverse=True)
  sorted_dict = {k: v for k, v in sorted_tuples}
  t_cnt=4
  result={}

  for i in sorted_dict.keys():
    if(t_cnt==4):
      pass
    elif(t_cnt==0):
      break
    else:
      result['Topic'+str(t_cnt)]=(Topics[i])
    t_cnt-=1
  return result

#print(topic_predictor("what is your marketing strategy"))

def create_reg(input_text):
	data= pd.read_csv('sentiment3data.csv') #change csv file name

	data['message'] = data['message'].apply(lambda x: " ".join(x.lower() for x in x.split()))
	#Removing Punctuation, Symbols
	data['message'] = data['message'].str.replace('[^\w\s]',' ')
	#Removing Stop Words using NLTK

	stop = stopwords.words('english')
	data['message'] = data['message'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
	#Lemmatisation

	data['message'] = data['message'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
	#Correcting Letter Repetitions

	def de_repeat(text):
		pattern = re.compile(r"(.)\1{2,}")
		return pattern.sub(r"\1\1", text)


	data['message'] = data['message'].apply(lambda x: " ".join(de_repeat(x) for x in x.split()))


	# Code to find the top 500 rarest words appearing in the data
	freq = pd.Series(' '.join(data['message']).split()).value_counts()[-500:]
	# Removing all those rarely appearing words from the data
	freq = list(freq.index)
	data['message'] = data['message'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))

	#Encoding output labels
	from sklearn import preprocessing
	lbl_enc = preprocessing.LabelEncoder()
	y = lbl_enc.fit_transform(data.sentiment.values)

	# Splitting into training and testing data in 80:20 ratio
	from sklearn.model_selection import train_test_split
	X_train, X_test, y_train, y_test = train_test_split(data.message.values, y, stratify=y, random_state=42, test_size=0.2, shuffle=True)


	count_vect = CountVectorizer(analyzer='word')
	count_vect.fit(data['message'])
	X_train_count =  count_vect.transform(X_train)
	X_test_count =  count_vect.transform(X_test)

	logreg = LogisticRegression(C=1)
	logreg.fit(X_train_count, y_train)
	#return logreg


	statements = pd.DataFrame([input_text])
	statements[0]= statements[0].str.replace('[^\w\s]',' ')
	stop = stopwords.words('english')
	statements[0] = statements[0].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
	statements[0] = statements[0].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
	# Extracting Count Vectors feature from our statements
	statements_count = count_vect.transform(statements[0])
	#Predicting the emotion of the statements using our already trained Logistic regression
	statements_prob = logreg.predict_proba(statements_count)
	statements_pred = logreg.predict(statements_count)
	print(statements_prob[0])
	labels=['Happy', 'Neutral', 'Sad']
	senti_op={}
	for i in range(3):
		senti_op[labels[i]]=statements_prob[0][i]

	return senti_op
				


#pip freeze > requirements.txt